# Machine Learning Methodology

## Overview

ScriptPulse vNext.4 employs Machine Learning (ML) strictly as a descriptive instrument. We explicitly reject end-to-end "black box" prediction of script quality, success, or "goodness." Instead, ML is used to map complex, high-dimensional narrative text into bounded, interpretable signals of first-pass cognitive load.

## 1. Information-Theoretic Surprisal (Cognitive Load)
Our primary measure of linguistic processing strain goes beyond simple sentence length (Flesch-Kincaid). We model the cognitive effort required to process text using Information-Theoretic Surprisal.

- **Model:** Next-token perplexity via GPT-2 (or equivalent auto-regressive transformer).
- **Mechanism:** The model calculates the negative log-likelihood of the scene's text sequence. Higher perplexity indicates less predictable text, which correlates strongly with higher cognitive load during a first read.
- **Graceful Degradation:** If the model is unavailable or encounters OOM errors, the system falls back to a deterministic Lexical Entropy baseline (Shannon Entropy over word frequencies).

## 2. Zero-Shot Thematic Resonance
Themes are culturally determined and cannot be objectively extracted from text. However, we can measure the *similarity* of a scene's content to predefined thematic anchors.

- **Model:** Sentence-BERT (`sentence-transformers/all-MiniLM-L6-v2`).
- **Mechanism:** We embed both the scene text (up to a token limit) and descriptions of core themes (e.g., "Sacrifice: giving up something precious"). We calculate the cosine similarity between the scene vector and theme vectors. Score thresholds (>0.35) determine if a theme is "resonating."
- **Fallback:** If SBERT fails, the system uses keyword-based heuristics as a transparent proxy.

## 3. Zero-Shot Emotional Dimension Mapping
ScriptPulse maps emotional arcs without pretending to "feel" them. We categorize emotional valence against Plutchik's Wheel of Emotions.

- **Model:** Zero-Shot Classification (`valhalla/distilbart-mnli-12-3` or similar NLI-based formulation).
- **Mechanism:** We formulate the scene text as the premise and the emotion labels (Joy, Fear, Anger, etc.) as the hypotheses. The returned probabilities represent the structural distribution of emotional markers, *not* an assessment of emotional impact.
- **Compound States:** Plutchik compound emotions (e.g., Joy + Trust = Love) are conditionally extracted based on overlapping probabilities.
- **Fallback:** Lexical heuristic sets (keyword hits) if inference fails.

## 4. Supervised Threshold Calibration (Future Work)
ScriptPulse includes the architectural scaffolding for supervised calibration of its alert thresholds. 

- **Strategy:** Collect ordinal labels from human first-pass readers describing *experiential difficulty* (e.g., "had to reread", "felt tiring"). 
- **Application:** Use shallow, interpretable models (e.g., Logistic Regression) to adjust the sensitivity thresholds of alarms generated by the unsupervised S(t) formula.
- **Strict Limitation:** Supervised learning is *never* used to map structural features to quality scores. Models are restricted to bounded threshold adjustments to prevent evaluative drift.

## Model Governance and Robustness
- **Determinism:** All pipelines use greedy decoding/fixed seeds where applicable to maintain reproducibility.
- **Version Pinning:** Models and their dependencies (e.g., `torch==2.1.2`, `transformers==4.36.2`) are strictly pinned in `requirements.txt` to guarantee stable behavior across environments.
- **Graceful Degradation:** Every ML component has a non-ML, heuristic fallback. The system logs warnings instead of failing silently or halting execution.

## References
- Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. *EMNLP*.
- Radford, A., et al. (2019). Language Models are Unsupervised Multitask Learners. *OpenAI*.
- Yin, W., Hay, J., & Roth, D. (2019). Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach. *EMNLP*.
